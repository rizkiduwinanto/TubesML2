{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar Pembelajaran Mesin 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muhammad Rizki Duwinanto - 13515006<br/>\n",
    "Kevin Erdiza Yogatama - 13515016<br/>\n",
    "Edwin Rachman - 13515042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pustaka Terkait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "test_data = [(x, y) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a. Create a Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deskripsi Algoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisialisasi model neural network ini menggunakan langkah-langkah sebagai berikut:\n",
    "1. Jumlah node setiap layer (parameter sizes) dispesifikasikan.\n",
    "2. Bias dan weight untuk setiap layer setelah layer pertama dihitung secara acak.\n",
    "\n",
    "Proses fitting model sebelumnya menggunakan algoritma sebagai berikut:\n",
    "1. Dalam fungsi fit dibutuhkan parameter jumlah data training (training_data), jumlah epoch (epochs), ukuran mini-batch (mini_batch_size), dan learning rate (learning_rate). Momentum (parameter momentum) jika diinginkan juga dapat dispesifikasikan. Data validasi (parameter validation_data) dapat dispesifikasikan secara langsung atau didapatkan dari training data jika parameter validation_split dispesifikasikan.\n",
    "2. Training data dibagi-bagi menjadi mini-batch berdasarkan ukuran mini-batch.\n",
    "3. Weight dan bias baru dihitung per mini-batch menggunakan fungsi update_mini_batch yang mengimplementasikan stochastic gradient. Dalam implementasi stochastic gradient digunakan fungsi backpropagation.\n",
    "4. Lakukan langkah 3 untuk setiap mini-batch\n",
    "5. Hitung akurasi dan loss training menggunakan fungsi evaluate dengan input mini-batch pertama. Fungsi evaluate menghasilkan nilai akurasi dan loss berdasarkan hasil feed-forward sigmoid (fungsi feed_forward). Hasil feed-forward setiap data jika lebih dari 0.5 akan menghasilkan 1, jika tidak menghasilkan 0. Akurasi dihitung dari rasio jumlah data setelah feed-forward yang sama dengan labelnya dengan jumlah seluruh data. Loss dihitung dari selisih kuadrat antara label dengan hasil data setelah feed-forward setiap data dibagi dengan jumlah semua data.\n",
    "6. Hitung akurasi dan loss validasi menggunakan fungsi evaluate dengan input validation_data.\n",
    "7. Lakukan langkah 2-6 untuk setiap epoch\n",
    "\n",
    "Untuk melakukan prediksi dapat digunakan fungsi predict dengan input test_data. Dalam fungsi ini setiap test_data akan dilakukan feed-forward seperti pada fungsi evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Code Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.biases = [np.random.rand(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.array([np.random.uniform(-0.05, 0.05, x) for i in range(0,y)]) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.history = d = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\n",
    "    \n",
    "    def feed_forward(self, activation):\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            activation = sigmoid(np.dot(weight, activation) + bias.transpose()[0])\n",
    "        return activation\n",
    "    \n",
    "    def fit(self, training_data, epochs, mini_batch_size, learning_rate,\n",
    "            momentum=0, validation_data=None, validation_split=0.0, verbose=1):\n",
    "        if validation_split != 0.0 and not validation_data :\n",
    "            training_data, validation_data = train_test_split(training_data, test_size=validation_split, random_state=42)\n",
    "        n_training = len(training_data)\n",
    "        if validation_data or validation_split != 0.0: \n",
    "            n_validation = len(validation_data)\n",
    "            if verbose != 0:\n",
    "                print(\"Train on {} samples, validate on {} samples\".format(n_training, n_validation))\n",
    "        for epoch in range(epochs):\n",
    "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, n_training, mini_batch_size)]\n",
    "            previous_weights = self.weights\n",
    "            previous_biases =self.biases\n",
    "            first = True\n",
    "            for mini_batch in mini_batches:\n",
    "                if first: previous_weights, previous_biases = self.weights, self.biases\n",
    "                start = time.time()\n",
    "                previous_weights, previous_biases = self.update_mini_batch(mini_batch, \n",
    "                                                                           learning_rate,\n",
    "                                                                           momentum, \n",
    "                                                                           previous_weights, \n",
    "                                                                           previous_biases)\n",
    "                end = time.time() - start\n",
    "            if validation_data or validation_split != 0:\n",
    "                training_accuracy, training_loss = self.evaluate(mini_batches[0])\n",
    "                validation_accuracy, validation_loss = self.evaluate(validation_data)\n",
    "                self.history['acc'].append(training_accuracy)\n",
    "                self.history['val_acc'].append(validation_accuracy)\n",
    "                self.history['loss'].append(training_loss)\n",
    "                self.history['val_loss'].append(validation_loss)\n",
    "                if verbose == 1 :\n",
    "                    print(\"Epoch {}/{} : {} s - loss: {} - acc: {} - val_loss: {} - val_acc: {}\".format(epoch + 1, \n",
    "                                                                                                        epochs,\n",
    "                                                                                                        end,\n",
    "                                                                                                        training_loss,\n",
    "                                                                                                        training_accuracy,\n",
    "                                                                                                        validation_loss,\n",
    "                                                                                                        validation_accuracy))\n",
    "                elif verbose == 2 :\n",
    "                    print(\"Epoch {} complete.\".format(epoch + 1))\n",
    "            else :\n",
    "                if verbose != 0:\n",
    "                    print(\"Epoch {} complete.\".format(epoch + 1))\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, learning_rate, momentum, previous_weights, previous_biases):\n",
    "        nabla_biases = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_weights = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_bias, delta_nabla_weights = self.backpropagation(x, y)\n",
    "            nabla_biases = [nb + dnb for nb, dnb in zip(nabla_biases, delta_nabla_bias)]\n",
    "            nabla_weights = [nw + dnw for nw, dnw in zip(nabla_weights, delta_nabla_weights)]\n",
    "        temp_weights = self.weights\n",
    "        temp_biases = self.biases\n",
    "        self.weights = [w + momentum * pw + (learning_rate/len(mini_batch)) * nw \n",
    "                        for w, nw, pw in zip(self.weights, nabla_weights, previous_weights)]\n",
    "        self.biases = [b + momentum * pb + (learning_rate/len(mini_batch)) * nb \n",
    "                       for b, nb, pb in zip(self.biases, nabla_biases, previous_biases)]\n",
    "        return (temp_weights, temp_biases)\n",
    "        \n",
    "    def backpropagation(self, x, y):\n",
    "        nabla_bias = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_weights = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        \n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        z_vectors = []\n",
    "        \n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation) + bias.transpose()[0]\n",
    "            z_vectors.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z_vectors[-1])\n",
    "        nabla_bias[-1] = delta\n",
    "        delta_newaxis = delta[:, np.newaxis]\n",
    "        m = len(activations[-2])\n",
    "        activations_newaxis = activations[-2][:, np.newaxis].reshape(1, m)\n",
    "        nabla_weights[-1] = np.dot(delta_newaxis, activations_newaxis)\n",
    "        \n",
    "        for layer in range(2, self.num_layers):\n",
    "            z = z_vectors[-layer]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-layer+1].transpose(), delta) * sp\n",
    "            nabla_bias[-layer] = delta\n",
    "            delta_newaxis = delta[:, np.newaxis]\n",
    "            m = len(activations[-layer-1].transpose())\n",
    "            activations_newaxis = activations[-layer-1].transpose()[:, np.newaxis].reshape(1, m)\n",
    "            nabla_weights[-layer] = np.dot(delta_newaxis, activations_newaxis)\n",
    "        \n",
    "        return (nabla_bias, nabla_weights)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(1 if self.feed_forward(x) * 2 >= 1 else 0, y) for x, y in test_data]\n",
    "        accuracy = sum(int(x == y) for x, y in test_results)/len(test_results)\n",
    "        loss = sum(math.pow((y - x), 2) for x, y in test_results)/len(test_results)\n",
    "        return accuracy, loss\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        test_results = [1 if self.feed_forward(x) * 2 >= 1 else 0 for x, y in test_data]\n",
    "        return (test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return np.squeeze(y - output_activations)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cara Pengunaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = Network([4, 5, 10, 1])\n",
    "neural_network.fit(train_data, 50, 5, 0.1, momentum=0.0001, validation_split=0.25, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah contoh penggunaan algoritma pada data latih iris. Namun, hasil tidak ditampilkan karena iris memiliki 3 label sedangkan classifier yang kami buat hanya biner. Untuk hasil classifier dapat dilihat pada bagian 1.b.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b.1 Explorasi Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deskripsi Algoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembelajaran akan menggunakan kakas keras dengan model <i>sequential</i> dan lapisan <i>dense</i> .Model akan memakai input layer sebanyak 1 neuron dengan bentuk input 4 sesuai jumlah attribute data latih, kemudian dengan 3 hidden layer masing-masing 2, 3, 4 neuron dan 1 output layer dengan 1 neuron. Optimizer yang dipakai adalah SGD, dengan perhitungan loss dengan Mean Squared Error, dan Metrics Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Code Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edwin\\Downloads\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Dense(1, activation='sigmoid', input_shape=(4,)))\n",
    "network.add(Dense(2, activation='sigmoid'))\n",
    "network.add(Dense(3, activation='sigmoid'))\n",
    "network.add(Dense(4, activation='sigmoid'))\n",
    "network.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 39\n",
      "Trainable params: 39\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Percobaan pada iris</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = network.fit(X_train, y_train, epochs=200, verbose=0, batch_size=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = network.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(score[0]*100.0))\n",
    "print(\"Accuracy {} %\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b.2 Eksperimen Data Categorization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b.2.1 Persiapan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hal pertama yang kami lakukan adalah menggunakan data latih Weather Categorization dari WEKA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('dataset/weather.csv')\n",
    "\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat data latih terdiri dari data numerik dan data kategorikal. Diperlukan preprocessing dengan kakas scikit-learn yaitu LabelEncoder sebagai berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "weather_df['outlook'] = label_encoder.fit_transform(weather_df.outlook)\n",
    "weather_df['windy'] = label_encoder.fit_transform(weather_df.windy)\n",
    "weather_df['play'] = label_encoder.fit_transform(weather_df.play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weather = weather_df.iloc[:,:4].values\n",
    "X_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_weather = weather_df.play.values\n",
    "y_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian, setelah kami menjadikan data latih tersebut numerik, kami melakukan pemisahan sebagian data latih (10%) menjadi data uji dengan proporsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_weather, y_weather, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "test_data = [(x, y) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b.2.2 Batch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1.b.2.2.1 Classifier Sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network1 = Network([4, 10, 8, 1])\n",
    "neural_network1.fit(train_data, 100, 1, 0.1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network1.history['acc'])\n",
    "plt.plot(neural_network1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network1.history['loss'])\n",
    "plt.plot(neural_network1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1, loss1 = neural_network1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(loss1*100.0))\n",
    "print(\"Accuracy {} %\".format(accuracy1*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.b.2.2.2 Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialisasi model keras untuk eksperimen pertama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1 = Sequential([\n",
    "    Dense(10, activation='sigmoid', input_shape=(4,)),\n",
    "    Dense(8, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = network1.fit(X_train, y_train, epochs=100, batch_size=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['acc'])\n",
    "plt.plot(history1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = network1.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(score1[0]*100.0))\n",
    "print(\"Accuracy {} %\".format(score1[1]*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b.2.3 Batch =  Jumlah Data Latih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.b.2.3.1 Classifier Sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network2 = Network([4, 1])\n",
    "neural_network2.fit(train_data, 100, len(X_train), 0.1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network2.history['acc'])\n",
    "plt.plot(neural_network2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network1.history['loss'])\n",
    "plt.plot(neural_network1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, loss2 = neural_network2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(loss2*100.0))\n",
    "print(\"Accuracy {} %\".format(accuracy2*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.b.2.3.2 Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialisasi model keras untuk eksperimen kedua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2 = Sequential([\n",
    "    Dense(10, activation='sigmoid', input_shape=(4,)),\n",
    "    Dense(8, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = network2.fit(X_train, y_train, epochs=100, batch_size=len(X_train), validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = network2.evaluate(X_test, y_test, batch_size=len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(score2[0]*100.0))\n",
    "print(\"Accuracy {} %\".format(score2[1]*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b.2.4 Analisis Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil eksperimen dengan classifier sendiri dan model keras, dapat disimpulkan dengan tabel berikut ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame({'Classifier' : ['Classifier Sendiri', 'Keras Model', 'Classifier Sendiri', 'Keras Model'],\n",
    "              'Batch' : [1, 1, len(X_train), len(X_train)],\n",
    "              'Loss' :[loss1, score1[0], loss2, score2[0]],\n",
    "              'Accuracy' :[accuracy1, score1[1], accuracy2, score2[1]]})\n",
    "analysis_df.index += 1\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akurasi yang dihasilkan dari kedua classifier adalah sama. Namun, jika melihat grafik dari akurasi dan loss di setiap cluster, terlihat bahwa classifier 4, dengan model keras dan batch = 12 atau sama dengan jumlah data, menghasilkan model yang lebih baik dari yang lain. Jika dibandingkan, proses pembelajaran berjalan lebih baik di keras dikarenakan lebih teroptimasi daripada algoritma kami, sehingga memiliki performa yang lebih baik walaupun tidak memakai adam. \n",
    "\n",
    "Sehingga dapat disimpulkan bahwa classifier terbaik dalam eksperimen ini adalah classifier 4 dengan model keras dan batch = 12 atau jumlah data. Hal lain yang dapat disim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Kerja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muhammad Rizki Duwinanto - 13515006 : Algoritma, Keras, Laporan <br/>\n",
    "Kevin Erdiza Yogatama - 13515016 : Algoritma, Keras, Laporan <br/>\n",
    "Edwin Rachman - 13515042 : Algoritma, Keras, Laporan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
