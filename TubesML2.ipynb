{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar Pembelajaran Mesin 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pustaka Terkait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a. Create a Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deskripsi Algoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisialisasi model neural network ini menggunakan langkah-langkah sebagai berikut:\n",
    "1. Jumlah node setiap layer (parameter sizes) dispesifikasikan.\n",
    "2. Bias dan weight untuk setiap layer setelah layer pertama dihitung secara acak.\n",
    "\n",
    "Proses fitting model sebelumnya menggunakan algoritma sebagai berikut:\n",
    "1. Dalam fungsi fit dibutuhkan parameter jumlah data training (training_data), jumlah epoch (epochs), ukuran mini-batch (mini_batch_size), dan learning rate (learning_rate). Momentum (parameter momentum) jika diinginkan juga dapat dispesifikasikan. Data validasi (parameter validation_data) dapat dispesifikasikan secara langsung atau didapatkan dari training data jika parameter validation_split dispesifikasikan.\n",
    "2. Training data dibagi-bagi menjadi mini-batch berdasarkan ukuran mini-batch.\n",
    "3. Weight dan bias baru dihitung per mini-batch menggunakan fungsi update_mini_batch yang mengimplementasikan stochastic gradient. Dalam implementasi stochastic gradient digunakan fungsi backpropagation.\n",
    "4. Lakukan langkah 3 untuk setiap mini-batch\n",
    "5. Hitung akurasi dan loss training menggunakan fungsi evaluate dengan input mini-batch pertama. Fungsi evaluate menghasilkan nilai akurasi dan loss berdasarkan hasil feed-forward sigmoid (fungsi feed_forward). Hasil feed-forward setiap data jika lebih dari 0.5 akan menghasilkan 1, jika tidak menghasilkan 0. Akurasi dihitung dari rasio jumlah data setelah feed-forward yang sama dengan labelnya dengan jumlah seluruh data. Loss dihitung dari selisih kuadrat antara label dengan hasil data setelah feed-forward setiap data dibagi dengan jumlah semua data.\n",
    "6. Hitung akurasi dan loss validasi menggunakan fungsi evaluate dengan input validation_data.\n",
    "7. Lakukan langkah 2-6 untuk setiap epoch\n",
    "\n",
    "Untuk melakukan prediksi dapat digunakan fungsi predict dengan input test_data. Dalam fungsi ini setiap test_data akan dilakukan feed-forward seperti pada fungsi evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Code Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.biases = [np.random.rand(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.rand(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.history = d = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\n",
    "    \n",
    "    def feed_forward(self, activation):\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            activation = sigmoid(np.dot(weight, activation) + bias.transpose()[0])\n",
    "        return activation\n",
    "    \n",
    "    def fit(self, training_data, epochs, mini_batch_size, learning_rate,\n",
    "            momentum=0, validation_data=None, validation_split=0.0, verbose=1):\n",
    "        if validation_split != 0.0 and not validation_data :\n",
    "            training_data, validation_data = train_test_split(training_data, test_size=validation_split, random_state=42)\n",
    "        n_training = len(training_data)\n",
    "        if validation_data or validation_split != 0.0: \n",
    "            n_validation = len(validation_data)\n",
    "            if verbose != 0:\n",
    "                print(\"Train on {} samples, validate on {} samples\".format(n_training, n_validation))\n",
    "        for epoch in range(epochs):\n",
    "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, n_training, mini_batch_size)]\n",
    "            previous_weights = self.weights\n",
    "            previous_biases =self.biases\n",
    "            first = True\n",
    "            for mini_batch in mini_batches:\n",
    "                if first: previous_weights, previous_biases = self.weights, self.biases\n",
    "                start = time.time()\n",
    "                previous_weights, previous_biases = self.update_mini_batch(mini_batch, \n",
    "                                                                           learning_rate,\n",
    "                                                                           momentum, \n",
    "                                                                           previous_weights, \n",
    "                                                                           previous_biases)\n",
    "                end = time.time() - start\n",
    "            if validation_data or validation_split != 0:\n",
    "                training_accuracy, training_loss = self.evaluate(mini_batches[0])\n",
    "                validation_accuracy, validation_loss = self.evaluate(validation_data)\n",
    "                self.history['acc'].append(training_accuracy)\n",
    "                self.history['val_acc'].append(validation_accuracy)\n",
    "                self.history['loss'].append(training_loss)\n",
    "                self.history['val_loss'].append(validation_loss)\n",
    "                if verbose == 1 :\n",
    "                    print(\"Epoch {}/{} : {} s - loss: {} - acc: {} - val_loss: {} - val_acc: {}\".format(epoch + 1, \n",
    "                                                                                                        epochs,\n",
    "                                                                                                        end,\n",
    "                                                                                                        training_loss,\n",
    "                                                                                                        training_accuracy,\n",
    "                                                                                                        validation_loss,\n",
    "                                                                                                        validation_accuracy))\n",
    "                elif verbose == 2 :\n",
    "                    print(\"Epoch {} complete.\".format(epoch + 1))\n",
    "            else :\n",
    "                if verbose != 0:\n",
    "                    print(\"Epoch {} complete.\".format(epoch + 1))\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, learning_rate, momentum, previous_weights, previous_biases):\n",
    "        nabla_biases = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_weights = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_bias, delta_nabla_weights = self.backpropagation(x, y)\n",
    "            nabla_biases = [nb + dnb for nb, dnb in zip(nabla_biases, delta_nabla_bias)]\n",
    "            nabla_weights = [nw + dnw for nw, dnw in zip(nabla_weights, delta_nabla_weights)]\n",
    "        temp_weights = self.weights\n",
    "        temp_biases = self.biases\n",
    "        self.weights = [w + momentum * pw + (learning_rate/len(mini_batch)) * nw \n",
    "                        for w, nw, pw in zip(self.weights, nabla_weights, previous_weights)]\n",
    "        self.biases = [b + momentum * pb + (learning_rate/len(mini_batch)) * nb \n",
    "                       for b, nb, pb in zip(self.biases, nabla_biases, previous_biases)]\n",
    "        return (temp_weights, temp_biases)\n",
    "        \n",
    "    def backpropagation(self, x, y):\n",
    "        nabla_bias = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_weights = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        \n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        z_vectors = []\n",
    "        \n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation) + bias.transpose()[0]\n",
    "            z_vectors.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z_vectors[-1])\n",
    "        nabla_bias[-1] = delta\n",
    "        delta_newaxis = delta[:, np.newaxis]\n",
    "        m = len(activations[-2])\n",
    "        activations_newaxis = activations[-2][:, np.newaxis].reshape(1, m)\n",
    "        nabla_weights[-1] = np.dot(delta_newaxis, activations_newaxis)\n",
    "        \n",
    "        for layer in range(2, self.num_layers):\n",
    "            z = z_vectors[-layer]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-layer+1].transpose(), delta) * sp\n",
    "            nabla_bias[-layer] = delta\n",
    "            delta_newaxis = delta[:, np.newaxis]\n",
    "            m = len(activations[-layer-1].transpose())\n",
    "            activations_newaxis = activations[-layer-1].transpose()[:, np.newaxis].reshape(1, m)\n",
    "            nabla_weights[-layer] = np.dot(delta_newaxis, activations_newaxis)\n",
    "        \n",
    "        return (nabla_bias, nabla_weights)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(1 if self.feed_forward(x) * 2 >= 1 else 0, y) for x, y in test_data]\n",
    "        accuracy = sum(int(x == y) for x, y in test_results)/len(test_results)\n",
    "        loss = sum(pow(int(y - x), 2) for x, y in test_results)/len(test_results)\n",
    "        return accuracy, loss\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        test_results = [1 if self.feed_forward(x) * 2 >= 1 else 0 for x, y in test_data]\n",
    "        return (test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return np.squeeze(y - output_activations)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b.1 Explorasi Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deskripsi Algoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembelajaran akan menggunakan kakas keras dengan model <i>sequential</i> dan lapisan <i>dense</i> .Model akan memakai input layer sebanyak 1 neuron dengan bentuk input 4 sesuai jumlah attribute data latih, kemudian dengan 3 hidden layer masing-masing 2 neuron kemudian 3 neuron, dan 4 neuron dan 1 output layer dengan 1 neuron. Optimizer yang dipakai adalah SGD, dengan perhitungan loss dengan Mean Squared Error, dan Metrics Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Code Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Dense(4, activation='sigmoid', input_shape=(4,)))\n",
    "network.add(Dense(5, activation='sigmoid'))\n",
    "network.add(Dense(5, activation='sigmoid'))\n",
    "network.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b.2 Eksperimen Data Categorization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persiapan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hal pertama yang kami lakukan adalah menggunakan data latih Weather Categorization dari WEKA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>83</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>68</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rainy</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>overcast</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sunny</td>\n",
       "      <td>72</td>\n",
       "      <td>95</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sunny</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rainy</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sunny</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>overcast</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overcast</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rainy</td>\n",
       "      <td>71</td>\n",
       "      <td>91</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outlook  temperature  humidity  windy play\n",
       "0      sunny           85        85  False   no\n",
       "1      sunny           80        90   True   no\n",
       "2   overcast           83        86  False  yes\n",
       "3      rainy           70        96  False  yes\n",
       "4      rainy           68        80  False  yes\n",
       "5      rainy           65        70   True   no\n",
       "6   overcast           64        65   True  yes\n",
       "7      sunny           72        95  False   no\n",
       "8      sunny           69        70  False  yes\n",
       "9      rainy           75        80  False  yes\n",
       "10     sunny           75        70   True  yes\n",
       "11  overcast           72        90   True  yes\n",
       "12  overcast           81        75  False  yes\n",
       "13     rainy           71        91   True   no"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df = pd.read_csv('dataset/weather.csv')\n",
    "\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat data latih terdiri dari data numerik dan data kategorikal. Diperlukan preprocessing dengan kakas scikit-learn yaitu LabelEncoder sebagai berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "weather_df['outlook'] = label_encoder.fit_transform(weather_df.outlook)\n",
    "weather_df['windy'] = label_encoder.fit_transform(weather_df.windy)\n",
    "weather_df['play'] = label_encoder.fit_transform(weather_df.play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook  temperature  humidity  windy  play\n",
       "0         2           85        85      0     0\n",
       "1         2           80        90      1     0\n",
       "2         0           83        86      0     1\n",
       "3         1           70        96      0     1\n",
       "4         1           68        80      0     1\n",
       "5         1           65        70      1     0\n",
       "6         0           64        65      1     1\n",
       "7         2           72        95      0     0\n",
       "8         2           69        70      0     1\n",
       "9         1           75        80      0     1\n",
       "10        2           75        70      1     1\n",
       "11        0           72        90      1     1\n",
       "12        0           81        75      0     1\n",
       "13        1           71        91      1     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 85, 85,  0],\n",
       "       [ 2, 80, 90,  1],\n",
       "       [ 0, 83, 86,  0],\n",
       "       [ 1, 70, 96,  0],\n",
       "       [ 1, 68, 80,  0],\n",
       "       [ 1, 65, 70,  1],\n",
       "       [ 0, 64, 65,  1],\n",
       "       [ 2, 72, 95,  0],\n",
       "       [ 2, 69, 70,  0],\n",
       "       [ 1, 75, 80,  0],\n",
       "       [ 2, 75, 70,  1],\n",
       "       [ 0, 72, 90,  1],\n",
       "       [ 0, 81, 75,  0],\n",
       "       [ 1, 71, 91,  1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_weather = weather_df.iloc[:,:4].values\n",
    "X_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_weather = weather_df.play.values\n",
    "y_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian, setelah kami menjadikan data latih tersebut numerik, kami melakukan pemisahan sebagian data latih (10%) menjadi data uji dengan proporsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_weather, y_weather, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 85, 85,  0],\n",
       "       [ 2, 80, 90,  1],\n",
       "       [ 0, 83, 86,  0],\n",
       "       [ 1, 70, 96,  0],\n",
       "       [ 1, 68, 80,  0],\n",
       "       [ 1, 65, 70,  1],\n",
       "       [ 0, 64, 65,  1],\n",
       "       [ 2, 72, 95,  0],\n",
       "       [ 2, 69, 70,  0],\n",
       "       [ 1, 75, 80,  0],\n",
       "       [ 2, 75, 70,  1],\n",
       "       [ 0, 72, 90,  1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 81, 75,  0],\n",
       "       [ 1, 71, 91,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "test_data = [(x, y) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Batch = 1 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 2 samples\n",
      "Epoch 1/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 2/500 : 0.0001246929168701172 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 3/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 4/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 5/500 : 0.00013828277587890625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 6/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 7/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 8/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 9/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 10/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 11/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 12/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 13/500 : 0.00012946128845214844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 14/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 15/500 : 0.0001239776611328125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 16/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 17/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 18/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 19/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 20/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 21/500 : 0.00013399124145507812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 22/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 23/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 24/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 25/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 26/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 27/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 28/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 29/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 30/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 31/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 32/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 33/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 34/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 35/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 36/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 37/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 38/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 39/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 40/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 41/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 42/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 43/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 44/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 45/500 : 0.00013017654418945312 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 46/500 : 0.0001304149627685547 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 47/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 48/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 49/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 50/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 51/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 52/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 53/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 54/500 : 0.0001347064971923828 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 55/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 56/500 : 0.0002472400665283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 57/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 58/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 59/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 60/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 61/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 62/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 63/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 64/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 65/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 66/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 67/500 : 0.00013184547424316406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 68/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 69/500 : 0.0005247592926025391 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 70/500 : 0.00021910667419433594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 71/500 : 0.00021839141845703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 72/500 : 0.0002033710479736328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 73/500 : 0.0002052783966064453 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 74/500 : 0.00019788742065429688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 75/500 : 0.00021076202392578125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 76/500 : 0.00011277198791503906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 77/500 : 0.00012993812561035156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 78/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 79/500 : 0.0001201629638671875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 80/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 81/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 82/500 : 0.0007307529449462891 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 83/500 : 0.00013136863708496094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 84/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 85/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 86/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 87/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 88/500 : 0.0003676414489746094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 89/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 90/500 : 0.00012969970703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 91/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 92/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 93/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 94/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 95/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 96/500 : 0.0002465248107910156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 97/500 : 0.00011777877807617188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 98/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 99/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 100/500 : 0.00011324882507324219 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 101/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 102/500 : 0.00012993812561035156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 103/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 104/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 105/500 : 0.0003733634948730469 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 106/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 107/500 : 0.00011444091796875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 108/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 109/500 : 0.00015616416931152344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 110/500 : 0.00021576881408691406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 111/500 : 0.00020456314086914062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 112/500 : 0.00022268295288085938 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 113/500 : 0.0002167224884033203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500 : 0.0001614093780517578 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 115/500 : 0.00011754035949707031 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 116/500 : 0.00012946128845214844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 117/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 118/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 119/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 120/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 121/500 : 0.00011420249938964844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 122/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 123/500 : 0.0003142356872558594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 124/500 : 0.00011610984802246094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 125/500 : 0.00015044212341308594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 126/500 : 0.00011563301086425781 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 127/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 128/500 : 0.0001366138458251953 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 129/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 130/500 : 0.00011372566223144531 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 131/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 132/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 133/500 : 0.00011229515075683594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 134/500 : 0.00012922286987304688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 135/500 : 0.00011324882507324219 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 136/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 137/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 138/500 : 0.00012946128845214844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 139/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 140/500 : 0.0001246929168701172 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 141/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 142/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 143/500 : 0.00022864341735839844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 144/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 145/500 : 0.00012183189392089844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 146/500 : 0.0001201629638671875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 147/500 : 0.00011491775512695312 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 148/500 : 0.00011467933654785156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 149/500 : 0.00014519691467285156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 150/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 151/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 152/500 : 0.00011301040649414062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 153/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 154/500 : 0.0001380443572998047 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 155/500 : 0.00011444091796875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 156/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 157/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 158/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 159/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 160/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 161/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 162/500 : 0.00011420249938964844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 163/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 164/500 : 0.00011801719665527344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 165/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 166/500 : 0.00011348724365234375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 167/500 : 0.0001647472381591797 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 168/500 : 0.0001437664031982422 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 169/500 : 0.0001285076141357422 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 170/500 : 0.00011277198791503906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 171/500 : 0.00011301040649414062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 172/500 : 0.00011706352233886719 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 173/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 174/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 175/500 : 0.00013065338134765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 176/500 : 0.00016021728515625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 177/500 : 0.0001678466796875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 178/500 : 0.00018978118896484375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 179/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 180/500 : 0.0001461505889892578 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 181/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 182/500 : 0.00013136863708496094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 183/500 : 0.00011396408081054688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 184/500 : 0.00011229515075683594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 185/500 : 0.0001373291015625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 186/500 : 0.00011992454528808594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 187/500 : 0.00012087821960449219 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 188/500 : 0.00012135505676269531 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 189/500 : 0.00014400482177734375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 190/500 : 0.0002491474151611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 191/500 : 0.00012874603271484375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 192/500 : 0.0001392364501953125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 193/500 : 0.00013208389282226562 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 194/500 : 0.00012302398681640625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 195/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 196/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 197/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 198/500 : 0.00015735626220703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 199/500 : 0.00015926361083984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 200/500 : 0.00015807151794433594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 201/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 202/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 203/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 204/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 205/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 206/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 207/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 208/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 209/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 210/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 211/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 212/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 213/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 214/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 215/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 216/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 217/500 : 0.00012946128845214844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 218/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 219/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 220/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 221/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 222/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 223/500 : 0.0002884864807128906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 224/500 : 0.0002510547637939453 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 225/500 : 0.00011968612670898438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 226/500 : 0.00022077560424804688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 227/500 : 0.00019812583923339844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/500 : 0.0001518726348876953 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 229/500 : 0.00011086463928222656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 230/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 231/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 232/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 233/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 234/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 235/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 236/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 237/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 238/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 239/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 240/500 : 0.00013828277587890625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 241/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 242/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 243/500 : 0.00011491775512695312 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 244/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 245/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 246/500 : 0.00015473365783691406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 247/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 248/500 : 0.00016736984252929688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 249/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 250/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 251/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 252/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 253/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 254/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 255/500 : 0.0002741813659667969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 256/500 : 0.0001285076141357422 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 257/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 258/500 : 0.00016641616821289062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 259/500 : 0.0001366138458251953 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 260/500 : 0.0001685619354248047 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 261/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 262/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 263/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 264/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 265/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 266/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 267/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 268/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 269/500 : 0.0001361370086669922 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 270/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 271/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 272/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 273/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 274/500 : 0.00014829635620117188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 275/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 276/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 277/500 : 0.00016164779663085938 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 278/500 : 0.00016307830810546875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 279/500 : 0.00013899803161621094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 280/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 281/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 282/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 283/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 284/500 : 0.00015544891357421875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 285/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 286/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 287/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 288/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 289/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 290/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 291/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 292/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 293/500 : 0.00023889541625976562 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 294/500 : 0.0003256797790527344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 295/500 : 0.0003223419189453125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 296/500 : 0.00011157989501953125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 297/500 : 0.00011372566223144531 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 298/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 299/500 : 0.00011229515075683594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 300/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 301/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 302/500 : 0.000110626220703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 303/500 : 0.000110626220703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 304/500 : 0.00011229515075683594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 305/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 306/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 307/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 308/500 : 0.00011157989501953125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 309/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 310/500 : 0.00011277198791503906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 311/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 312/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 313/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 314/500 : 0.00011706352233886719 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 315/500 : 0.00014495849609375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 316/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 317/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 318/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 319/500 : 0.00011157989501953125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 320/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 321/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 322/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 323/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 324/500 : 0.00011110305786132812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 325/500 : 0.00011181831359863281 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 326/500 : 0.000110626220703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 327/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 328/500 : 0.00011134147644042969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 329/500 : 0.00015044212341308594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 330/500 : 0.00017952919006347656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 331/500 : 0.000141143798828125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 332/500 : 0.00011086463928222656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 333/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 334/500 : 0.00011014938354492188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 335/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 336/500 : 0.00020623207092285156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 337/500 : 0.00020933151245117188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 338/500 : 0.00011444091796875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 339/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 340/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 341/500 : 0.0001289844512939453 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 342/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 343/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 344/500 : 0.00011014938354492188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 345/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 346/500 : 0.00023651123046875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 347/500 : 0.00011444091796875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 348/500 : 0.0001323223114013672 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 349/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 351/500 : 0.00013446807861328125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 352/500 : 0.00021219253540039062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 353/500 : 0.00011563301086425781 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 354/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 355/500 : 0.00011897087097167969 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 356/500 : 0.00011920928955078125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 357/500 : 0.0001125335693359375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 358/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 359/500 : 0.00011205673217773438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 360/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 361/500 : 0.0001354217529296875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 362/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 363/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 364/500 : 0.0001304149627685547 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 365/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 366/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 367/500 : 0.00022912025451660156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 368/500 : 0.00020813941955566406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 369/500 : 0.00022411346435546875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 370/500 : 0.00022077560424804688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 371/500 : 0.00020647048950195312 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 372/500 : 0.000133514404296875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 373/500 : 0.00013756752014160156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 374/500 : 0.00011324882507324219 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 375/500 : 0.00011301040649414062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 376/500 : 0.00011348724365234375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 377/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 378/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 379/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 380/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 381/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 382/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 383/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 384/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 385/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 386/500 : 0.00017142295837402344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 387/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 388/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 389/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 390/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 391/500 : 0.00013637542724609375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 392/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 393/500 : 0.00014734268188476562 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 394/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 395/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 396/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 397/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 398/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 399/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 400/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 401/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 402/500 : 0.00012922286987304688 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 403/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 404/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 405/500 : 0.0001323223114013672 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 406/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 407/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 408/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 409/500 : 0.0001289844512939453 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 410/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 411/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 412/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 413/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 414/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 415/500 : 0.00013637542724609375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 416/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 417/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 418/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 419/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 420/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 421/500 : 0.000133514404296875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 422/500 : 0.0002105236053466797 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 423/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 424/500 : 0.00012946128845214844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 425/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 426/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 427/500 : 0.00013566017150878906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 428/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 429/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 430/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 431/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 432/500 : 0.00013446807861328125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 433/500 : 0.00025343894958496094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 434/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 435/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 436/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 437/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 438/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 439/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 440/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 441/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 442/500 : 0.00012755393981933594 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 443/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 444/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 445/500 : 0.00013065338134765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 446/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 447/500 : 0.00012803077697753906 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 448/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 449/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 450/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 451/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 452/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 453/500 : 0.00012826919555664062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 454/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 455/500 : 0.0001277923583984375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 456/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 457/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 458/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 459/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 460/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 461/500 : 0.0002200603485107422 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 462/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 463/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 464/500 : 0.00012493133544921875 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 465/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 466/500 : 0.0001888275146484375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 467/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 468/500 : 0.0001266002655029297 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 469/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 471/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 472/500 : 0.00012683868408203125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 473/500 : 0.0001304149627685547 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 474/500 : 0.0001289844512939453 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 475/500 : 0.0003383159637451172 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 476/500 : 0.00013184547424316406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 477/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 478/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 479/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 480/500 : 0.0001347064971923828 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 481/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 482/500 : 0.0001246929168701172 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 483/500 : 0.000125885009765625 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 484/500 : 0.0001251697540283203 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 485/500 : 0.00012421607971191406 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 486/500 : 0.00012731552124023438 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 487/500 : 0.00012540817260742188 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 488/500 : 0.00012636184692382812 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 489/500 : 0.00012564659118652344 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 490/500 : 0.00012612342834472656 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 491/500 : 0.0003685951232910156 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 492/500 : 0.00013971328735351562 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 493/500 : 0.0015635490417480469 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 494/500 : 0.00011420249938964844 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 495/500 : 0.00012969970703125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 496/500 : 0.00021076202392578125 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 497/500 : 0.0001270771026611328 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 498/500 : 0.00018167495727539062 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 499/500 : 0.00013136863708496094 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 500/500 : 0.00012874603271484375 s - loss: 1.0 - acc: 0.0 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "neural_network1 = Network([4, 10, 8, 1])\n",
    "neural_network1.fit(train_data, 500, 1, 0.1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGpBJREFUeJzt3XuYFfWd5/H3hwZsErnIJWpopFHJaDNGQnq97xoTYkAT2SejUVYfFTG9zsbLrHEyuOOowUxGM7mMETYZEknUMTKoY5ZkcPCayWaNgVbbCyCxZVCaYGiI4GVEaPnuH1Vdc2ib7gN09ek+5/N6nvN01a9+p873h+35dP3qnCpFBGZmZgADSl2AmZn1HQ4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBSsIkiqlRSSBhbR92JJv+qNusz6GoeC9TmS1knaIWl0h/Zn0jf22tJUZlb+HArWV/0bMLN9RdIxwAdKV07fUMyRjtn+cChYX3UXcGHB+kXAnYUdJA2XdKekVkmvSLpO0oB0W5Wkb0raLGktcGYnz71d0kZJGyR9TVJVMYVJulfSa5K2SfqlpEkF24ZI+lZazzZJv5I0JN12iqQnJG2VtF7SxWn7LyRdWrCP3aav0qOjL0l6CXgpbbs13ccbkp6S9J8L+ldJ+l+SXpb0Zrp9nKT5kr7VYSxLJP3PYsZtlcGhYH3Vk8AwSUenb9bnAf/Qoc9twHDgcOBUkhCZlW77IvBZ4GNAPXB2h+f+GGgDjkz7nA5cSnEeBCYCHwKeBu4u2PZN4OPAScBI4CvALknj0+fdBowBJgNNRb4ewH8Fjgfq0vUV6T5GAj8B7pVUnW67muQo6wxgGHAJ8O/AHcDMguAcDUxNn2+WiAg//OhTD2AdyZvVdcDfANOAh4GBQAC1QBWwA6greN5/B36RLj8GXFaw7fT0uQOBg4F3gSEF22cCj6fLFwO/KrLWEel+h5P8kfUOcGwn/a4FHtjDPn4BXFqwvtvrp/v/ZDd1vN7+usAaYMYe+q0GPp0uXw4sLfV/bz/61sPzk9aX3QX8EphAh6kjYDQwCHiloO0VYGy6/GFgfYdt7canz90oqb1tQIf+nUqPWv4aOIfkL/5dBfUcAFQDL3fy1HF7aC/WbrVJugaYTTLOIDkiaD8x39Vr3QFcQBKyFwC37kdNVoY8fWR9VkS8QnLC+Qzgnzps3gzsJHmDb3cYsCFd3kjy5li4rd16kiOF0RExIn0Mi4hJdO+/ATNIjmSGkxy1ACitaTtwRCfPW7+HdoC32f0k+iGd9MkuZ5yeP/gK8AXgoIgYAWxLa+jutf4BmCHpWOBo4Kd76GcVyqFgfd1skqmTtwsbI+I9YDHw15KGpnP2V/Mf5x0WA1dKqpF0EDCn4LkbgYeAb0kaJmmApCMknVpEPUNJAmULyRv51wv2uwtYCHxb0ofTE74nSjqA5LzDVElfkDRQ0ihJk9OnNgGfl/QBSUemY+6uhjagFRgo6XqSI4V2PwRukjRRiY9KGpXW2EJyPuIu4P6IeKeIMVsFcShYnxYRL0dE4x42X0HyV/Za4FckJ0wXptt+ACwDniU5GdzxSONCYDCwimQ+/j7g0CJKupNkKmpD+twnO2y/Bnie5I33D8AtwICIeJXkiOfLaXsTcGz6nO+QnB/5Pcn0zt10bRnwL8Bv01q2s/v00rdJQvEh4A3gdmBIwfY7gGNIgsFsN4rwTXbMKomk/0JyRDU+/AZgHfhIwayCSBoEXAX80IFgnXEomFUISUcDW0mmyf6uxOVYH+XpIzMzy/hIwczMMv3uy2ujR4+O2traUpdhZtavPPXUU5sjYkx3/fpdKNTW1tLYuKdPKJqZWWckvdJ9L08fmZlZAYeCmZllHApmZpbpd+cUOrNz505aWlrYvn17qUvpNdXV1dTU1DBo0KBSl2JmZaQsQqGlpYWhQ4dSW1tLwaWQy1ZEsGXLFlpaWpgwYUKpyzGzMpLb9JGkhZI2SXphD9sl6buSmiU9J2nKvr7W9u3bGTVqVEUEAoAkRo0aVVFHRmbWO/I8p/Bjkjtm7cl0klsaTgQagO/tz4tVSiC0q7TxmlnvyG36KCJ+Kam2iy4zgDvTi3I9KWmEpEPTa933vG0tsLPMLh3/1ib40TWlrsLMesshx8D0m3N9iVKeUxjL7teAb0nb3hcKkhpIjiY47LDDOm4uuS1/eJ1Pff4iAF7btJmqqgGMGTUSgOUP3cfgwYO73cesK+Yw56oG/ujIw3Ot1cysK/3iRHNELAAWANTX1+/bFfyG1/RkSbsZNRqaXlgNwI033siBBx7INdfs/hd8+02xBwzofMbuR/fcv/cv3NoGs/55759nZrYHpfyewgZ2v4duDf9xf92y0NzcTF1dHeeffz6TJk1i48aNNDQ0UF9fz6RJk5g7d27W95RTTqGpqYm2tjZGjBjBnDlzOPbYYznxxBPZtGlTCUdhZpWklEcKS4DLJS0Cjge29cT5hK/+bCWrfvfGfhdXqO7Dw7jhc8Xc0/39XnzxRe68807q6+sBuPnmmxk5ciRtbW2cdtppnH322dTV1e32nG3btnHqqady8803c/XVV7Nw4ULmzJnT2e7NzHpUnh9JvQf4NfBHklokzZZ0maTL0i5LSe6t20xyP93/kVctpXTEEUdkgQBwzz33MGXKFKZMmcLq1atZtWrV+54zZMgQpk+fDsDHP/5x1q1b11vlmlmFy/PTRzO72R7Al3r6dff1L/q8fPCDH8yWX3rpJW699VaWL1/OiBEjuOCCCzr9rkHhiemqqira2tp6pVYzM1/7qBe98cYbDB06lGHDhrFx40aWLVtW6pLMzHbTLz59VC6mTJlCXV0dRx11FOPHj+fkk08udUlmZrvpd/dorq+vj4432Vm9ejVHH310iSoqnUodt5ntPUlPRUR9d/08fWRmZhmHgpmZZRwKZmaWcSiYmVnGoWBmZhmHgpmZZRwKPWDLli1MnjyZyZMnc8ghhzB27NhsfceOHUXvZ+HChbz22ms5Vmpm1jV/ea0HjBo1iqamJmDPl84uxsKFC5kyZQqHHHJIT5doZlYUh0LO7rjjDubPn8+OHTs46aSTmDdvHrt27WLWrFk0NTURETQ0NHDwwQfT1NTEueeey5AhQ1i+fHlRN+cxM+tJ5RcKD86B157v2X3u4y3wXnjhBR544AGeeOIJBg4cSENDA4sWLeKII45g8+bNPP98UufWrVsZMWIEt912G/PmzWPy5Mk9W7+ZWZHKLxT6kEceeYQVK1Zkl85+5513GDduHJ/5zGdYs2YNV155JWeeeSann356iSs1M0uUXyjkfFPrvRERXHLJJdx0003v2/bcc8/x4IMPMn/+fO6//34WLFhQggrNzHbnTx/laOrUqSxevJjNmzcDyaeUXn31VVpbW4kIzjnnHObOncvTTz8NwNChQ3nzzTdLWbKZVbjyO1LoQ4455hhuuOEGpk6dyq5duxg0aBDf//73qaqqYvbs2UQEkrjlllsAmDVrFpdeeqlPNJtZyfjS2f1YpY7bzPaeL51tZmZ7zaFgZmaZsgmF/jYNtr8qbbxm1jvKIhSqq6vZsmVLxbxRRgRbtmyhurq61KWYWZkpi08f1dTU0NLSQmtra6lL6TXV1dXU1NSUugwzKzNlEQqDBg1iwoQJpS7DzKzfK4vpIzMz6xkOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMwsk2soSJomaY2kZklzOtl+mKTHJT0j6TlJZ+RZj5mZdS23UJBUBcwHpgN1wExJdR26XQcsjoiPAecB/zuveszMrHt5HikcBzRHxNqI2AEsAmZ06BPAsHR5OPC7HOsxM7Nu5BkKY4H1BestaVuhG4ELJLUAS4ErOtuRpAZJjZIaK+n6RmZmva3UJ5pnAj+OiBrgDOAuSe+rKSIWRER9RNSPGTOm14s0M6sUeYbCBmBcwXpN2lZoNrAYICJ+DVQDo3OsyczMupBnKKwAJkqaIGkwyYnkJR36vAp8CkDS0SSh4PkhM7MSyS0UIqINuBxYBqwm+ZTRSklzJZ2Vdvsy8EVJzwL3ABdHpdwpx8ysD8r1fgoRsZTkBHJh2/UFy6uAk/OswczMilfqE81mZtaHOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCyTayhImiZpjaRmSXP20OcLklZJWinpJ3nWY2ZmXRuY144lVQHzgU8DLcAKSUsiYlVBn4nAtcDJEfG6pA/lVY+ZmXUvzyOF44DmiFgbETuARcCMDn2+CMyPiNcBImJTjvWYmVk3ug0FSVdIOmgf9j0WWF+w3pK2FfoI8BFJ/0/Sk5Km7aGGBkmNkhpbW1v3oRQzMytGMUcKB5NM/SxOzxGoB19/IDAR+AQwE/iBpBEdO0XEgoioj4j6MWPG9ODLm5lZoW5DISKuI3njvh24GHhJ0tclHdHNUzcA4wrWa9K2Qi3AkojYGRH/Bvw2fS0zMyuBos4pREQAr6WPNuAg4D5J3+jiaSuAiZImSBoMnAcs6dDnpyRHCUgaTTKdtHZvBmBmZj2n208fSboKuBDYDPwQ+POI2ClpAPAS8JXOnhcRbZIuB5YBVcDCiFgpaS7QGBFL0m2nS1oFvJfue0tPDMzMzPZeMR9JHQl8PiJeKWyMiF2SPtvVEyNiKbC0Q9v1BcsBXJ0+zMysxIqZPnoQ+EP7iqRhko4HiIjVeRVmZma9r5hQ+B7wVsH6W2mbmZmVmWJCQek0D5BMG5HjN6HNzKx0igmFtZKulDQofVyFPyFkZlaWigmFy4CTSL5j0AIcDzTkWZSZmZVGt9NA6fWIzuuFWszMrMSK+Z5CNTAbmARUt7dHxCU51mVmZiVQzPTRXcAhwGeAfyW5XMWbeRZlZmalUUwoHBkRfwW8HRF3AGeSnFcwM7MyU0wo7Ex/bpX0x8BwwDfDMTMrQ8V832BBej+F60guaHcg8Fe5VmVmZiXRZSikF717I70z2i+Bw3ulKjMzK4kup4/Sby93ehVUMzMrP8WcU3hE0jWSxkka2f7IvTIzM+t1xZxTODf9+aWCtsBTSWZmZaeYbzRP6I1CzMys9Ir5RvOFnbVHxJ09X46ZmZVSMdNH/6lguRr4FPA04FAwMyszxUwfXVG4LmkEsCi3iszMrGSK+fRRR28DPs9gZlaGijmn8DOSTxtBEiJ1wOI8izIzs9Io5pzCNwuW24BXIqIlp3rMzKyEigmFV4GNEbEdQNIQSbURsS7XyszMrNcVc07hXmBXwfp7aZuZmZWZYkJhYETsaF9JlwfnV5KZmZVKMaHQKums9hVJM4DN+ZVkZmalUsw5hcuAuyXNS9dbgE6/5WxmZv1bMV9eexk4QdKB6fpbuVdlZmYl0e30kaSvSxoREW9FxFuSDpL0td4ozszMelcx5xSmR8TW9pX0Lmxn5FeSmZmVSjGhUCXpgPYVSUOAA7rob2Zm/VQxoXA38Kik2ZIuBR4G7ihm55KmSVojqVnSnC76/YmkkFRfXNlmZpaHYk403yLpWWAqyTWQlgHju3uepCpgPvBpkk8srZC0JCJWdeg3FLgK+M3el29mZj2p2Kuk/p4kEM4BPgmsLuI5xwHNEbE2/cLbImBGJ/1uAm4BthdZi5mZ5WSPoSDpI5JukPQicBvJNZAUEadFxLw9Pa/AWGB9wXpL2lb4GlOAcRHxz13tSFKDpEZJja2trUW8tJmZ7YuujhReJDkq+GxEnBIRt5Fc96hHSBoAfBv4cnd9I2JBRNRHRP2YMWN6qgQzM+ugq1D4PLAReFzSDyR9CtBe7HsDMK5gvSZtazcU+GPgF5LWAScAS3yy2cysdPYYChHx04g4DzgKeBz4M+BDkr4n6fQi9r0CmChpgqTBwHnAkoL9b4uI0RFRGxG1wJPAWRHRuB/jMTOz/dDtieaIeDsifhIRnyP5a/8Z4C+KeF4bcDnJp5VWA4sjYqWkuYUX2DMzs75DEdF9rz6kvr4+Ght9MGFmtjckPRUR3U7PF/uRVDMzqwAOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzy+QaCpKmSVojqVnSnE62Xy1plaTnJD0qaXye9ZiZWddyCwVJVcB8YDpQB8yUVNeh2zNAfUR8FLgP+EZe9ZiZWffyPFI4DmiOiLURsQNYBMwo7BARj0fEv6erTwI1OdZjZmbdyDMUxgLrC9Zb0rY9mQ082NkGSQ2SGiU1tra29mCJZmZWqE+caJZ0AVAP/G1n2yNiQUTUR0T9mDFjerc4M7MKMjDHfW8AxhWs16Rtu5E0FfhL4NSIeDfHeszMrBt5HimsACZKmiBpMHAesKSwg6SPAX8PnBURm3KsxczMipBbKEREG3A5sAxYDSyOiJWS5ko6K+32t8CBwL2SmiQt2cPuzMysF+Q5fURELAWWdmi7vmB5ap6vb2Zme6dPnGg2M7O+waFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWVyDQVJ0yStkdQsaU4n2w+Q9I/p9t9Iqs2zHjMz61puoSCpCpgPTAfqgJmS6jp0mw28HhFHAt8BbsmrHjMz697AHPd9HNAcEWsBJC0CZgCrCvrMAG5Ml+8D5klSRERPF/PVn61k1e/e6Ondmpn1mroPD+OGz03K9TXynD4aC6wvWG9J2zrtExFtwDZgVMcdSWqQ1CipsbW1NadyzcwszyOFHhMRC4AFAPX19ft0FJF3upqZlYM8jxQ2AOMK1mvStk77SBoIDAe25FiTmZl1Ic9QWAFMlDRB0mDgPGBJhz5LgIvS5bOBx/I4n2BmZsXJbfooItokXQ4sA6qAhRGxUtJcoDEilgC3A3dJagb+QBIcZmZWIrmeU4iIpcDSDm3XFyxvB87JswYzMyuev9FsZmYZh4KZmWUcCmZmlnEomJlZRv3tE6CSWoFX9vHpo4HNPVhOf+AxVwaPuTLsz5jHR8SY7jr1u1DYH5IaI6K+1HX0Jo+5MnjMlaE3xuzpIzMzyzgUzMwsU2mhsKDUBZSAx1wZPObKkPuYK+qcgpmZda3SjhTMzKwLDgUzM8tUTChImiZpjaRmSXNKXU9PkbRQ0iZJLxS0jZT0sKSX0p8Hpe2S9N303+A5SVNKV/m+kzRO0uOSVklaKemqtL1sxy2pWtJySc+mY/5q2j5B0m/Ssf1jepl6JB2Qrjen22tLWf++klQl6RlJP0/Xy3q8AJLWSXpeUpOkxrSt1363KyIUJFUB84HpQB0wU1JdaavqMT8GpnVomwM8GhETgUfTdUjGPzF9NADf66Uae1ob8OWIqANOAL6U/vcs53G/C3wyIo4FJgPTJJ0A3AJ8JyKOBF4HZqf9ZwOvp+3fSfv1R1cBqwvWy3287U6LiMkF30novd/tiCj7B3AisKxg/Vrg2lLX1YPjqwVeKFhfAxyaLh8KrEmX/x6Y2Vm//vwA/g/w6UoZN/AB4GngeJJvtw5M27Pfc5L7mJyYLg9M+6nUte/lOGvSN8BPAj8HVM7jLRj3OmB0h7Ze+92uiCMFYCywvmC9JW0rVwdHxMZ0+TXg4HS57P4d0mmCjwG/oczHnU6lNAGbgIeBl4GtEdGWdikcVzbmdPs2YFTvVrzf/g74CrArXR9FeY+3XQAPSXpKUkPa1mu/27neZMdKLyJCUll+7ljSgcD9wJ9FxBuSsm3lOO6IeA+YLGkE8ABwVIlLyo2kzwKbIuIpSZ8odT297JSI2CDpQ8DDkl4s3Jj373alHClsAMYVrNekbeXq95IOBUh/bkrby+bfQdIgkkC4OyL+KW0u+3EDRMRW4HGS6ZMRktr/uCscVzbmdPtwYEsvl7o/TgbOkrQOWEQyhXQr5TveTERsSH9uIgn/4+jF3+1KCYUVwMT0kwuDSe4FvaTENeVpCXBRunwRyZx7e/uF6ScWTgC2FRyS9htKDgluB1ZHxLcLNpXtuCWNSY8QkDSE5BzKapJwODvt1nHM7f8WZwOPRTrp3B9ExLURURMRtST/vz4WEedTpuNtJ+mDkoa2LwOnAy/Qm7/bpT6p0osnb84AfksyD/uXpa6nB8d1D7AR2EkynzibZC71UeAl4BFgZNpXJJ/Cehl4Hqgvdf37OOZTSOZdnwOa0scZ5Txu4KPAM+mYXwCuT9sPB5YDzcC9wAFpe3W63pxuP7zUY9iPsX8C+HkljDcd37PpY2X7e1Vv/m77MhdmZpaplOkjMzMrgkPBzMwyDgUzM8s4FMzMLONQMDOzjEPBrANJ76VXqGx/9NhVdSXVquCKtmZ9jS9zYfZ+70TE5FIXYVYKPlIwK1J6nftvpNe6Xy7pyLS9VtJj6fXsH5V0WNp+sKQH0nsgPCvppHRXVZJ+kN4X4aH0G8pmfYJDwez9hnSYPjq3YNu2iDgGmEdyFU+A24A7IuKjwN3Ad9P27wL/Gsk9EKaQfEMVkmvfz4+IScBW4E9yHo9Z0fyNZrMOJL0VEQd20r6O5EY3a9ML8r0WEaMkbSa5hv3OtH1jRIyW1ArURMS7BfuoBR6O5GYpSPoLYFBEfC3/kZl1z0cKZnsn9rC8N94tWH4Pn9uzPsShYLZ3zi34+et0+QmSK3kCnA/833T5UeBPIbtBzvDeKtJsX/kvFLP3G5Le4azdv0RE+8dSD5L0HMlf+zPTtiuAH0n6c6AVmJW2XwUskDSb5IjgT0muaGvWZ/mcglmR0nMK9RGxudS1mOXF00dmZpbxkYKZmWV8pGBmZhmHgpmZZRwKZmaWcSiYmVnGoWBmZpn/DwIb/kZEBBAKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(neural_network1.history['acc'])\n",
    "plt.plot(neural_network1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(neural_network1.history['loss'])\n",
    "plt.plot(neural_network1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1, loss1 = neural_network1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 50.0 %\n",
      "Accuracy 50.0 %\n",
      "Time: 0.9927120208740234 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: {} %\".format(loss1*100.0))\n",
    "print(\"Accuracy {} %\".format(accuracy1*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialisasi model keras untuk eksperimen pertama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1 = Sequential([\n",
    "    Dense(4, input_shape=(4,)),\n",
    "    Dense(10, activation='sigmoid'),\n",
    "    Dense(8, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 167\n",
      "Trainable params: 167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 2 samples\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.2603 - acc: 0.6000 - val_loss: 0.0732 - val_acc: 1.0000\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 856us/step - loss: 0.2582 - acc: 0.6000 - val_loss: 0.0755 - val_acc: 1.0000\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 943us/step - loss: 0.2562 - acc: 0.6000 - val_loss: 0.0784 - val_acc: 1.0000\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2547 - acc: 0.6000 - val_loss: 0.0810 - val_acc: 1.0000\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2538 - acc: 0.6000 - val_loss: 0.0835 - val_acc: 1.0000\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2530 - acc: 0.6000 - val_loss: 0.0857 - val_acc: 1.0000\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 819us/step - loss: 0.2523 - acc: 0.6000 - val_loss: 0.0879 - val_acc: 1.0000\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 935us/step - loss: 0.2516 - acc: 0.6000 - val_loss: 0.0903 - val_acc: 1.0000\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 958us/step - loss: 0.2509 - acc: 0.6000 - val_loss: 0.0926 - val_acc: 1.0000\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 916us/step - loss: 0.2503 - acc: 0.6000 - val_loss: 0.0951 - val_acc: 1.0000\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 822us/step - loss: 0.2496 - acc: 0.6000 - val_loss: 0.0975 - val_acc: 1.0000\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2491 - acc: 0.6000 - val_loss: 0.0999 - val_acc: 1.0000\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 803us/step - loss: 0.2484 - acc: 0.6000 - val_loss: 0.1025 - val_acc: 1.0000\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 919us/step - loss: 0.2479 - acc: 0.6000 - val_loss: 0.1047 - val_acc: 1.0000\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 894us/step - loss: 0.2473 - acc: 0.6000 - val_loss: 0.1067 - val_acc: 1.0000\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2465 - acc: 0.6000 - val_loss: 0.1085 - val_acc: 1.0000\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2459 - acc: 0.6000 - val_loss: 0.1102 - val_acc: 1.0000\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2454 - acc: 0.6000 - val_loss: 0.1120 - val_acc: 1.0000\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2450 - acc: 0.6000 - val_loss: 0.1136 - val_acc: 1.0000\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 945us/step - loss: 0.2447 - acc: 0.6000 - val_loss: 0.1151 - val_acc: 1.0000\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 818us/step - loss: 0.2444 - acc: 0.6000 - val_loss: 0.1166 - val_acc: 1.0000\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2442 - acc: 0.6000 - val_loss: 0.1181 - val_acc: 1.0000\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 888us/step - loss: 0.2440 - acc: 0.6000 - val_loss: 0.1195 - val_acc: 1.0000\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 829us/step - loss: 0.2437 - acc: 0.6000 - val_loss: 0.1210 - val_acc: 1.0000\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 879us/step - loss: 0.2435 - acc: 0.6000 - val_loss: 0.1223 - val_acc: 1.0000\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 805us/step - loss: 0.2433 - acc: 0.6000 - val_loss: 0.1237 - val_acc: 1.0000\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 831us/step - loss: 0.2431 - acc: 0.6000 - val_loss: 0.1250 - val_acc: 1.0000\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 967us/step - loss: 0.2430 - acc: 0.6000 - val_loss: 0.1262 - val_acc: 1.0000\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 933us/step - loss: 0.2429 - acc: 0.6000 - val_loss: 0.1274 - val_acc: 1.0000\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 868us/step - loss: 0.2427 - acc: 0.6000 - val_loss: 0.1286 - val_acc: 1.0000\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2426 - acc: 0.6000 - val_loss: 0.1297 - val_acc: 1.0000\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2424 - acc: 0.6000 - val_loss: 0.1309 - val_acc: 1.0000\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 940us/step - loss: 0.2423 - acc: 0.6000 - val_loss: 0.1320 - val_acc: 1.0000\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 795us/step - loss: 0.2422 - acc: 0.6000 - val_loss: 0.1332 - val_acc: 1.0000\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2421 - acc: 0.6000 - val_loss: 0.1342 - val_acc: 1.0000\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2420 - acc: 0.6000 - val_loss: 0.1351 - val_acc: 1.0000\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 925us/step - loss: 0.2419 - acc: 0.6000 - val_loss: 0.1361 - val_acc: 1.0000\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 805us/step - loss: 0.2418 - acc: 0.6000 - val_loss: 0.1370 - val_acc: 1.0000\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 816us/step - loss: 0.2418 - acc: 0.6000 - val_loss: 0.1380 - val_acc: 1.0000\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 961us/step - loss: 0.2417 - acc: 0.6000 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 822us/step - loss: 0.2416 - acc: 0.6000 - val_loss: 0.1396 - val_acc: 1.0000\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 966us/step - loss: 0.2416 - acc: 0.6000 - val_loss: 0.1405 - val_acc: 1.0000\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2416 - acc: 0.6000 - val_loss: 0.1412 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 862us/step - loss: 0.2415 - acc: 0.6000 - val_loss: 0.1420 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 803us/step - loss: 0.2415 - acc: 0.6000 - val_loss: 0.1427 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 832us/step - loss: 0.2414 - acc: 0.6000 - val_loss: 0.1433 - val_acc: 1.0000\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 936us/step - loss: 0.2413 - acc: 0.6000 - val_loss: 0.1441 - val_acc: 1.0000\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 844us/step - loss: 0.2414 - acc: 0.6000 - val_loss: 0.1447 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2413 - acc: 0.6000 - val_loss: 0.1453 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1460 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 902us/step - loss: 0.2413 - acc: 0.6000 - val_loss: 0.1465 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 956us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1470 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1475 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1479 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1484 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 958us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1488 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 838us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1493 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 895us/step - loss: 0.2412 - acc: 0.6000 - val_loss: 0.1498 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1502 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 924us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1506 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 791us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1510 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 822us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1514 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1517 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 899us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1521 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 818us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1524 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1528 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1531 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 960us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1533 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 881us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1536 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 919us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1539 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 876us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1542 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1544 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 859us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1547 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 959us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1549 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1552 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1554 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1556 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 939us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1557 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 880us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1559 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 841us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1560 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 899us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1562 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 805us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1563 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 898us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1565 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 845us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1567 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1570 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 850us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1571 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 941us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1573 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1575 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 958us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1577 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 979us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1577 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 995us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 958us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1579 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 854us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1579 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1579 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 793us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1581 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 953us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1582 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 861us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1583 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1583 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 842us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1583 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1584 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1584 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1586 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 880us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1587 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1589 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1588 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 875us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1589 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 816us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1591 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1590 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 843us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1592 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 792us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 988us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 981us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 893us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 814us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 846us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 905us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 810us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 845us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1592 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 911us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1592 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 867us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 939us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1593 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 831us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 944us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 910us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 901us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1596 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1596 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 839us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1596 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1597 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1596 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1597 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1597 - acc: 1.000 - 0s 990us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 840us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 904us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 882us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 902us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 980us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 831us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 873us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 796us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 960us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 894us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 812us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 819us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 871us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 862us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 844us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 822us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 955us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 879us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 884us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 898us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 793us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1597 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 906us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 970us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 813us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 849us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1597 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 981us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 914us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 864us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 882us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 913us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 879us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 920us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 961us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 832us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 998us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 833us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 849us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 984us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 907us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 928us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 861us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 892us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 805us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 837us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 975us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 894us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 894us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 868us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 890us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 991us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 802us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 848us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 810us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 966us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 908us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 870us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 930us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 815us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 858us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 847us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 925us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 853us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 833us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 869us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1607 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 860us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 810us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 839us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 853us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 928us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 876us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 853us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 854us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 860us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 841us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1603 - acc: 1.000 - 0s 978us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 845us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 826us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 884us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 1000us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 984us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 909us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 734us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 304/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 836us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 910us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 837us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 871us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 877us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 951us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 979us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 847us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 856us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 916us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 915us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 847us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 827us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 814us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 818us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 864us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 872us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 947us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 888us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 882us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 881us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 958us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 821us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 894us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 836us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 993us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 947us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 954us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 831us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 966us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 786us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 860us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 837us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 845us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 968us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 812us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 916us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 992us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 993us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 939us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 899us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 936us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 970us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 861us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 872us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 983us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 960us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 883us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 910us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 865us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 821us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 786us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 786us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 836us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 881us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 864us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 814us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1607 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1607 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 866us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1607 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 883us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 984us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 908us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 898us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 881us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 908us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 895us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 860us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1607 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 939us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 788us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 999us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 989us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 874us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 839us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 930us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 864us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 870us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 906us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 917us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 959us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 792us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 978us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 816us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 898us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 890us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 931us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 954us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 869us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 882us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 891us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 826us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 905us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 841us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 881us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 844us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 883us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 919us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 880us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 861us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 897us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 945us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 940us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2411 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 822us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 965us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 907us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 980us/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 841us/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2410 - acc: 0.6000 - val_loss: 0.1604 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history1 = network1.fit(X_train, y_train, epochs=500, batch_size=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['acc'])\n",
    "plt.plot(history1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "score1 = network1.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 26.01824328303337 %\n",
      "Accuracy 50.0 %\n",
      "Time: 5.225123167037964 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: {} %\".format(score1[0]*100.0))\n",
    "print(\"Accuracy {} %\".format(score1[1]*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Batch Size = Jumlah Data Latih</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 2 samples\n",
      "Epoch 1/500 : 0.0015017986297607422 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 2/500 : 0.0010921955108642578 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 3/500 : 0.0010199546813964844 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 4/500 : 0.0010116100311279297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 5/500 : 0.0009989738464355469 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 6/500 : 0.0010149478912353516 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 7/500 : 0.0010094642639160156 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 8/500 : 0.0009961128234863281 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 9/500 : 0.0011184215545654297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 10/500 : 0.0010066032409667969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 11/500 : 0.001256704330444336 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 12/500 : 0.0011107921600341797 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 13/500 : 0.0010116100311279297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 14/500 : 0.001020193099975586 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 15/500 : 0.0010013580322265625 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 16/500 : 0.0010094642639160156 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 17/500 : 0.001255035400390625 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 18/500 : 0.001256704330444336 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 19/500 : 0.0010421276092529297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 20/500 : 0.001001596450805664 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 21/500 : 0.0010030269622802734 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 22/500 : 0.0010747909545898438 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 23/500 : 0.0010097026824951172 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 24/500 : 0.0010221004486083984 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 25/500 : 0.0010082721710205078 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 26/500 : 0.0010023117065429688 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 27/500 : 0.001463174819946289 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 28/500 : 0.0016875267028808594 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 29/500 : 0.0010166168212890625 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 30/500 : 0.0010650157928466797 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 31/500 : 0.0010671615600585938 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 32/500 : 0.0010046958923339844 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 33/500 : 0.000997781753540039 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 34/500 : 0.0010650157928466797 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 35/500 : 0.001058816909790039 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 36/500 : 0.0011076927185058594 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 37/500 : 0.001165628433227539 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 38/500 : 0.0009908676147460938 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 39/500 : 0.0009968280792236328 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 40/500 : 0.0011065006256103516 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 41/500 : 0.0011150836944580078 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 42/500 : 0.0012035369873046875 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 43/500 : 0.0010025501251220703 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 44/500 : 0.0010023117065429688 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 45/500 : 0.0010128021240234375 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 46/500 : 0.0010018348693847656 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 47/500 : 0.0010066032409667969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 48/500 : 0.0010004043579101562 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 49/500 : 0.0010099411010742188 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 50/500 : 0.0010066032409667969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 51/500 : 0.0015521049499511719 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 52/500 : 0.0012660026550292969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 53/500 : 0.0010068416595458984 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 54/500 : 0.0009987354278564453 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 55/500 : 0.001024484634399414 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 56/500 : 0.0010061264038085938 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 57/500 : 0.0010137557983398438 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 58/500 : 0.0010027885437011719 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 59/500 : 0.0010106563568115234 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 60/500 : 0.001005411148071289 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 61/500 : 0.0010085105895996094 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 62/500 : 0.0010609626770019531 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 63/500 : 0.0010106563568115234 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 64/500 : 0.0010101795196533203 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 65/500 : 0.0010027885437011719 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 66/500 : 0.0010120868682861328 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 67/500 : 0.0010068416595458984 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 68/500 : 0.0010111331939697266 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 69/500 : 0.0010156631469726562 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 70/500 : 0.0010066032409667969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 71/500 : 0.0010018348693847656 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 72/500 : 0.0010104179382324219 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 73/500 : 0.0010101795196533203 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 74/500 : 0.0010137557983398438 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 75/500 : 0.001004934310913086 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 76/500 : 0.0010020732879638672 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 77/500 : 0.0010111331939697266 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 78/500 : 0.0010058879852294922 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 79/500 : 0.0010001659393310547 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 80/500 : 0.0010135173797607422 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 81/500 : 0.0010008811950683594 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 82/500 : 0.0011262893676757812 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 83/500 : 0.0010020732879638672 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 84/500 : 0.0010075569152832031 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 85/500 : 0.0010137557983398438 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 86/500 : 0.0010547637939453125 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 87/500 : 0.0009951591491699219 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 88/500 : 0.0010106563568115234 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 89/500 : 0.001001119613647461 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 90/500 : 0.0010080337524414062 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 91/500 : 0.0010166168212890625 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 92/500 : 0.0010564327239990234 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 93/500 : 0.001058816909790039 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 94/500 : 0.0009963512420654297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 95/500 : 0.0009984970092773438 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 96/500 : 0.0010721683502197266 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 97/500 : 0.0010056495666503906 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 98/500 : 0.001008749008178711 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 99/500 : 0.001020193099975586 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 100/500 : 0.0010111331939697266 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 101/500 : 0.0009996891021728516 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 102/500 : 0.001322031021118164 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 103/500 : 0.0010144710540771484 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 104/500 : 0.0010802745819091797 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 105/500 : 0.0010073184967041016 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 106/500 : 0.0010025501251220703 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 107/500 : 0.0010170936584472656 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 108/500 : 0.0010101795196533203 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 109/500 : 0.001003265380859375 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 110/500 : 0.001016855239868164 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 111/500 : 0.0010058879852294922 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 112/500 : 0.0010051727294921875 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 113/500 : 0.0010004043579101562 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 114/500 : 0.0010056495666503906 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 115/500 : 0.0009975433349609375 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 116/500 : 0.0009980201721191406 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 117/500 : 0.0010020732879638672 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 118/500 : 0.0010106563568115234 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 119/500 : 0.0010046958923339844 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 120/500 : 0.001001596450805664 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 121/500 : 0.001009225845336914 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 122/500 : 0.001005411148071289 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 123/500 : 0.0010097026824951172 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 124/500 : 0.0012333393096923828 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 125/500 : 0.001010894775390625 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 126/500 : 0.0010142326354980469 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 127/500 : 0.0010085105895996094 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 128/500 : 0.0010066032409667969 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 129/500 : 0.0010726451873779297 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 130/500 : 0.0010061264038085938 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 131/500 : 0.0010020732879638672 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 132/500 : 0.0010149478912353516 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 133/500 : 0.0010037422180175781 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n",
      "Epoch 134/500 : 0.0010099411010742188 s - loss: 0.4 - acc: 0.6 - val_loss: 0.0 - val_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "neural_network2 = Network([4, 10, 8, 1])\n",
    "neural_network2.fit(train_data, 500, len(X_train), 0.1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network2.history['acc'])\n",
    "plt.plot(neural_network2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_network2.history['acc'])\n",
    "plt.plot(neural_network2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, loss2 = neural_network2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(loss2*100.0))\n",
    "print(\"Accuracy {} %\".format(accuracy2*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialisasi model keras untuk eksperimen kedua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2 = Sequential([\n",
    "    Dense(4, input_shape=(4,)),\n",
    "    Dense(10, activation='sigmoid'),\n",
    "    Dense(8, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2.compile(optimizer='SGD', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history2 = network2.fit(X_train, y_train, epochs=500, batch_size=len(X_train), validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = network2.evaluate(X_test, y_test, batch_size=len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {} %\".format(score2[0]*100.0))\n",
    "print(\"Accuracy {} %\".format(score2[1]*100.0))\n",
    "print(\"Time: {} ms\".format(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Kerja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
